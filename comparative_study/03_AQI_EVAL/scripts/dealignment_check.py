# -*- coding: utf-8 -*-
"""DeAlignment_Check.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1stQBYN7sIIPCKt96XJLZLmuixF9Rv2Xq
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import AutoModel
from tqdm import tqdm
from sklearn.manifold import TSNE
from sklearn.metrics import davies_bouldin_score
!pip install datasets
from datasets import load_dataset
# !pip install -U bitsandbytes
# !pip install unsloth
from unsloth import FastLanguageModel

# # Load model and tokenizer
# MODEL_NAME = "abhilekhborah/de-alignment_llama-3.2-1b-25_pd"

# device = "cuda" if torch.cuda.is_available() else "cpu"
# # model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")

# # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

model_name = "unsloth/Llama-3.2-1B"
max_seq_length = 4096  # Adjust as needed
dtype = "bfloat16"  # Can be "float16", "bfloat16", or "float32"
load_in_4bit = True  # Set to False if not using 4-bit quantization

# Load model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

model.eval()  # Set model to evaluation mode


# Load dataset
ds = load_dataset("Anthropic/hh-rlhf")
#5k examples

device = "cuda" if torch.cuda.is_available() else "cpu"



def get_hidden_states_batch(texts, batch_size=15):
    """Processes text in batches and extracts hidden states."""
    hidden_states_list = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Processing batches"):
        batch_texts = texts[i : i + batch_size]
        inputs = tokenizer(batch_texts.tolist(), return_tensors="pt", padding=True, truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
            last_hidden_states = outputs.hidden_states[-1].mean(dim=1)  # Mean pooling over last layer

        hidden_states_list.extend(last_hidden_states.cpu().numpy())

    return np.array(hidden_states_list)

def cluster_and_visualize(df):
    """
    Clusters accepted and rejected hidden states using t-SNE and visualizes them in 3D.
    """
    accepted_embeddings = np.vstack(df["accepted_hidden_states"])
    rejected_embeddings = np.vstack(df["rejected_hidden_states"])

    # Combine embeddings and labels
    X = np.vstack((accepted_embeddings, rejected_embeddings))
    y = np.array([1] * len(accepted_embeddings) + [0] * len(rejected_embeddings))  # 1 for accepted, 0 for rejected

    # t-SNE reduction to 3D
    tsne = TSNE(n_components=3, perplexity=30, random_state=42, n_iter=5000)
    X_3D = tsne.fit_transform(X)

    # Compute Davies–Bouldin Score
    dbs_score = davies_bouldin_score(X_3D, y)
    print(f"Davies–Bouldin Score: {dbs_score:.4f}")

    # 3D Plot
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    ax.scatter(
        X_3D[y == 1, 0], X_3D[y == 1, 1], X_3D[y == 1, 2],
        c='green', label='Accepted', alpha=0.7
    )
    ax.scatter(
        X_3D[y == 0, 0], X_3D[y == 0, 1], X_3D[y == 0, 2],
        c='red', label='Rejected', alpha=0.7
    )

    ax.set_xlabel('t-SNE 1')
    ax.set_ylabel('t-SNE 2')
    ax.set_zlabel('t-SNE 3')
    ax.set_title(f'3D t-SNE Visualization of Response Clusters -- DBS_SCORE = {dbs_score:.4f}')
    ax.legend()
    plt.show()



df = pd.DataFrame(ds["train"]).head(2500)

df

# Process accepted and rejected text
df["accepted_hidden_states"] = list(get_hidden_states_batch(df["chosen"]))
df["rejected_hidden_states"] = list(get_hidden_states_batch(df["rejected"]))

# Save processed DataFrame
df.to_pickle("processed_data_aligned_lamma3.2_1b.pkl")
print("Hidden states extraction complete. Data saved as processed_data.pkl")

# Load processed data and run clustering
df = pd.read_pickle("processed_data_dealigned_lamma3.2_1b.pkl")
df2=pd.read_pickle("processed_data_aligned_lamma3.2_1b.pkl")
df=df.head(1000)
df2=df2.head(1000)

cluster_and_visualize(df)
cluster_and_visualize(df2)